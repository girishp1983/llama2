{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girishp1983/llama2/blob/master/categorical_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApebG9Ys30VF"
      },
      "source": [
        "# PyTorch Categorical VAE with Gumbel-Softmax\n",
        "\n",
        "This notebook shows how to train a VAE with categorical latents using the Gumbel-softmax trick. The accompanying blog post is here: https://jxmo.io/posts/variational-autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNZBFxiQ0iOw"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from typing import Tuple\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch.distributions as dist\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fJeVFSp0sAk"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    cnn: torch.nn.Module\n",
        "    input_shape: torch.Size\n",
        "    N: int # number of categorical distributions\n",
        "    K: int # number of classes\n",
        "    def __init__(self, N: int, K: int, input_shape: torch.Size):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.input_shape = input_shape\n",
        "        print('N =', N, 'and K =', K)\n",
        "        self.network = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(3 * 3 * 32, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, N*K),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Produces encoding `z` for input spectrogram `x`.\n",
        "\n",
        "        Actually returns theta, the parameters of a bernoulli producing `z`.\n",
        "        \"\"\"\n",
        "        assert len(x.shape) == 4 # x should be of shape [B, C, Y, X]\n",
        "        return self.network(x).view(-1, self.N, self.K)\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    output_shape: torch.Size\n",
        "    N: int # number of categorical distributions\n",
        "    K: int # number of classes\n",
        "    def __init__(self, N: int, K: int, output_shape: torch.Size):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.K = K\n",
        "        self.output_shape = output_shape\n",
        "        self.network = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(N*K, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 3 * 3 * 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
        "            torch.nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "            torch.nn.BatchNorm2d(16),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "            torch.nn.BatchNorm2d(8),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Produces output `x_hat` for input `z`.\n",
        "\n",
        "        z is a tensor with a batch dimension and, for each item,\n",
        "            containing parameters of N categorical distributions,\n",
        "            each with K classes\n",
        "        \"\"\"\n",
        "        assert len(z.shape) == 3 # [B, N, K]\n",
        "        assert z.shape[1:] == (self.N, self.K)\n",
        "        x_hat = self.network(z)\n",
        "        return x_hat.view((-1,) + self.output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqzHBRVV1E92"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def gumbel_distribution_sample(shape: torch.Size, eps=1e-20) -> torch.Tensor:\n",
        "    \"\"\"Samples from the Gumbel distribution given a tensor shape and value of epsilon.\n",
        "\n",
        "    note: the \\eps here is just for numerical stability. The code is basically just doing\n",
        "            > -log(-log(rand(shape)))\n",
        "    where rand generates random numbers on U(0, 1).\n",
        "    \"\"\"\n",
        "    U = torch.rand(shape)\n",
        "    return -torch.log(-torch.log(U + eps) + eps)\n",
        "\n",
        "def gumbel_softmax_distribution_sample(logits: torch.Tensor, temperature: float) -> torch.Tensor:\n",
        "    \"\"\"Adds Gumbel noise to `logits` and applies softmax along the last dimension.\n",
        "\n",
        "    Softmax is applied wrt a given temperature value. A higher temperature will make the softmax\n",
        "    softer (less spiky). Lower temperature will make softmax more spiky and less soft. As\n",
        "    temperature -> 0, this distribution approaches a categorical distribution.\n",
        "    \"\"\"\n",
        "    assert len(logits.shape) == 2 # (should be of shape (b, n_classes))\n",
        "    y = logits + gumbel_distribution_sample(logits.shape).to(device)\n",
        "    return torch.nn.functional.softmax(y / temperature, dim=-1)\n",
        "\n",
        "def gumbel_softmax(logits: torch.Tensor, temperature: float, batch=False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Gumbel-softmax.\n",
        "    input: [*, n_classes] (or [b, *, n_classes] for batch)\n",
        "    return: flatten --> [*, n_class] a one-hot vector (or b, *, n_classes for batch)\n",
        "    \"\"\"\n",
        "    input_shape = logits.shape\n",
        "    if batch:\n",
        "        assert len(logits.shape) == 3\n",
        "        b, n, k = input_shape\n",
        "        logits = logits.view(b*n, k)\n",
        "    assert len(logits.shape) == 2\n",
        "    y = gumbel_softmax_distribution_sample(logits, temperature)\n",
        "    n_classes = input_shape[-1] # TODO(jxm): check this!\n",
        "    return y.view(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeYYjelO0kDF"
      },
      "outputs": [],
      "source": [
        "class CategoricalVAE(torch.nn.Module):\n",
        "    encoder: torch.nn.Module\n",
        "    decoder: torch.nn.Module\n",
        "    temperature: float\n",
        "    def __init__(self, encoder: torch.nn.Module, decoder: torch.nn.Module):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def forward(self, x: torch.Tensor, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"VAE forward pass. Encoder produces phi, the parameters of a categorical distribution.\n",
        "        Samples from categorical(phi) using gumbel softmax to produce a z. Passes z through encoder p(x|z)\n",
        "        to get x_hat, a reconstruction of x.\n",
        "\n",
        "        Returns:\n",
        "            phi: parameters of categorical distribution that produced z\n",
        "            x_hat: auto-encoder reconstruction of x\n",
        "        \"\"\"\n",
        "        phi = self.encoder(x)\n",
        "        B, N, K = phi.shape\n",
        "\n",
        "        z_given_x = gumbel_softmax(phi, temperature, batch=True)\n",
        "        x_hat = self.decoder(z_given_x)\n",
        "        return phi, x_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tll6HtT1LJQ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "import torch.distributions as dist\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def load_training_data():\n",
        "    # TODO implement datasets better\n",
        "    # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    return torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "def categorical_kl_divergence(phi: torch.Tensor) -> torch.Tensor:\n",
        "    # phi is logits of shape [B, N, K] where B is batch, N is number of categorical distributions, K is number of classes\n",
        "    B, N, K = phi.shape\n",
        "    phi = phi.view(B*N, K)\n",
        "    q = dist.Categorical(logits=phi)\n",
        "    p = dist.Categorical(probs=torch.full((B*N, K), 1.0/K).to(device)) # uniform bunch of K-class categorical distributions\n",
        "    kl = dist.kl.kl_divergence(q, p) # kl is of shape [B*N]\n",
        "    return kl.view(B, N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1O2-vR12lOx",
        "outputId": "8283b5ad-1abb-4a03-e545-bddf754462bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N = 30 and K = 10\n"
          ]
        }
      ],
      "source": [
        "model_save_interval = 5_000\n",
        "batch_size = 64\n",
        "max_steps = 50_000\n",
        "initial_learning_rate = 0.001\n",
        "initial_temperature = 1.0\n",
        "minimum_temperature = 0.5\n",
        "temperature_anneal_rate = 0.00003\n",
        "K = 10 # number of classes\n",
        "N = 30 # number of categorical distributions\n",
        "\n",
        "training_images = load_training_data()\n",
        "train_dataset = torch.utils.data.DataLoader(\n",
        "    dataset=training_images,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "image_shape = next(iter(train_dataset))[0][0].shape # [1, 28, 28]\n",
        "encoder = Encoder(N, K, image_shape)\n",
        "decoder = Decoder(N, K, image_shape)\n",
        "model = CategoricalVAE(encoder, decoder)\n",
        "\n",
        "parameters = list(model.parameters())\n",
        "optimizer = optim.SGD(parameters, lr=initial_learning_rate, momentum=0.0)\n",
        "learning_rate_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
        "step = 0\n",
        "temperature = initial_temperature\n",
        "\n",
        "# make folder for images\n",
        "output_dir = os.path.join('outputs', 'categorical_vae')\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9o5C_h725Zi",
        "outputId": "506bbb0d-1ddb-4129-ea38-35f5e42998de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('using device:', device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxbstPsx2s0d",
        "outputId": "b74fec9a-ce5e-44bc-b404-b09e371e4dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training | Recon. loss = 106.8197479 / KL loss = 16.9503593:  25%|██▍       | 12332/50000 [03:49<11:14, 55.86it/s]"
          ]
        }
      ],
      "source": [
        "progress_bar = tqdm.tqdm(total=max_steps, desc='Training')\n",
        "while step < max_steps:\n",
        "    for data in train_dataset: # x should be a batch of torch.Tensor spectrograms, of shape [B, F, T]\n",
        "        x = data[0].to(device)\n",
        "        phi, x_hat = model(x, temperature) # phi shape: [B, N, K]; x_hat shape: [B, C, Y, X]\n",
        "        reconstruction_loss = (\n",
        "            torch.nn.functional.binary_cross_entropy(x_hat, x, reduction=\"none\").sum()) / x.shape[0]\n",
        "        kl_loss = torch.mean(\n",
        "            torch.sum(categorical_kl_divergence(phi), dim=1)\n",
        "        )\n",
        "        loss = kl_loss + reconstruction_loss\n",
        "        progress_bar.set_description(f'Training | Recon. loss = {reconstruction_loss:.7f} / KL loss = {kl_loss:.7f}')\n",
        "        gradnorm = torch.nn.utils.clip_grad_norm_(parameters, 1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Incrementally anneal temperature and learning rate.\n",
        "        if step % 1000 == 1:\n",
        "            temperature = np.maximum(initial_temperature*np.exp(-temperature_anneal_rate*step), minimum_temperature)\n",
        "            learning_rate_scheduler.step() # should multiply learning rate by 0.9\n",
        "\n",
        "        if (step+1) % model_save_interval == 0:\n",
        "            torch.save(model.state_dict(), os.path.join(output_dir, f'save_{step}.pt'))\n",
        "\n",
        "        step += 1\n",
        "        progress_bar.update(1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}